# Model Compression 
>Images of Model Compression
>![studentimg1](https://user-images.githubusercontent.com/89927101/205546600-d81e8ffa-ddf1-4842-83f6-53f89ee9065d.png)
![studentimg2](https://user-images.githubusercontent.com/89927101/205546607-2d720c74-833b-4060-9dc6-54766ad2dcba.png)
![studentimg3](https://user-images.githubusercontent.com/89927101/205546615-25c8bd51-5ef3-4672-8fd7-606977efff80.png)
![studentimg4](https://user-images.githubusercontent.com/89927101/205546622-bcae0bf9-2075-47c9-afc7-5a89038cd5de.png)
![studentimg5](https://user-images.githubusercontent.com/89927101/205546633-d5064d26-ca40-4487-89d1-2f027443811a.png)
![studentimg6](https://user-images.githubusercontent.com/89927101/205546638-70b68c23-fe2d-4f4f-9cae-09a5e43038f9.png)
![studentimg7](https://user-images.githubusercontent.com/89927101/205546643-58bd82a1-cc5f-477f-b1e2-cc8caaead7c6.png)
![studentimg8](https://user-images.githubusercontent.com/89927101/205546646-5d00749f-5506-47a8-98cb-e3a781949ec3.png)
![studentimg9](https://user-images.githubusercontent.com/89927101/205546651-c88a41bf-da77-4a17-a249-9aab9b1bd8dc.png)
![studentimg10](https://user-images.githubusercontent.com/89927101/205546658-6d4f299c-6442-472c-a097-21f29b59e7dc.png)
![student_training_validation_loss_10epochs](https://user-images.githubusercontent.com/89927101/205546681-15d2ab10-b12d-4581-907e-f35a374286e1.png)
![student_training_validation_iou_10epochs](https://user-images.githubusercontent.com/89927101/205546685-94cab8a0-b1d2-49aa-99ce-f82f4952993b.png)
![student_precision_recall_curve_10epochs](https://user-images.githubusercontent.com/89927101/205546693-bd1e3f18-5dee-41d2-bcb5-72a9ee23645a.png)
## Knowledge Distillation
>When working with large data and predicition models, it can be very costly to work with the data and make those 
>predictions and take a long time. Since other users might need to use this and not all users have similar computer performances so then model compressions are needed to allow it to be usable by other users that aren't the creator. This is possible with distilling knowledge in an ensemble of models into a single model. Knowledge distillation is distilling the knowledge in a neural network, and it is trained to copy larger model or models into one smaller model that can be redeployed under real-world constraints. Used more in neural network models to help itslef be redeployed without significant loss in performance.
